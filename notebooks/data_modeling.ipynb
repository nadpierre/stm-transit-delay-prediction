{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6efc2fa1",
   "metadata": {},
   "source": [
    "# STM Transit Delay Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48c78c",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4739c1",
   "metadata": {},
   "source": [
    "This notebook explores tree-based machine learning models in order to find the one that predicts STM transit delays with the best accuracy. The featured models are XGBoost, LightGBM and CatBoost, because they are more suitable for large datasets with mixed data and high cardinality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d218f9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2dded95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shap\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01d19b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (1500000, 22)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('../data/preprocessed.parquet')\n",
    "print(f'Shape of dataset: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc11b74",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51490206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target variable\n",
    "X = df.drop('delay', axis=1)\n",
    "y = df['delay']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4a098",
   "metadata": {},
   "source": [
    "The 3 models can run multiple iterations with a training and validation set. Therefore, a hold-out set will be kept to evaluate the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee2060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split (60-20-20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "del X_temp\n",
    "del y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f9998",
   "metadata": {},
   "source": [
    "**Scaling**\n",
    "\n",
    "Since only tree-based models are explored in this project, scaling is not needed because the models are not sensitive to the absolute scale or distribution of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a69b3",
   "metadata": {},
   "source": [
    "## Fit Base Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce7b6b",
   "metadata": {},
   "source": [
    "All models allow to setup a number of rounds and early stopping. To start, all models will run 100 rounds with an early stopping of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96f66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to track metrics\n",
    "metrics_df = pd.DataFrame(columns=['model', 'MAE', 'RMSE', 'R²'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14b6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reg_metrics(metrics_df:pd.DataFrame, y_pred:pd.Series, y_true:pd.Series, model_name:str) -> pd.DataFrame:\n",
    "\tmae = mean_absolute_error(y_true, y_pred)\n",
    "\trmse = root_mean_squared_error(y_true, y_pred)\n",
    "\tr2 = r2_score(y_true, y_pred)\n",
    "\n",
    "\tmetrics_df.loc[len(metrics_df)] = [model_name, mae, rmse, r2]\n",
    "\treturn metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f3056",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e10d4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "xg_train_data = xgb.DMatrix(X_train, y_train, enable_categorical=False)\n",
    "xg_val_data = xgb.DMatrix(X_val, y_val, enable_categorical=False)\n",
    "xg_eval_set = [(xg_train_data, 'train'), (xg_val_data, 'validation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8303fcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:156.42192\tvalidation-rmse:155.69049\n",
      "[10]\ttrain-rmse:147.61415\tvalidation-rmse:147.63489\n",
      "[20]\ttrain-rmse:145.51539\tvalidation-rmse:145.81676\n",
      "[30]\ttrain-rmse:143.52252\tvalidation-rmse:144.17869\n",
      "[40]\ttrain-rmse:142.16577\tvalidation-rmse:143.08907\n",
      "[50]\ttrain-rmse:140.78638\tvalidation-rmse:141.90729\n",
      "[60]\ttrain-rmse:139.90561\tvalidation-rmse:141.21075\n",
      "[70]\ttrain-rmse:138.69200\tvalidation-rmse:140.27174\n",
      "[80]\ttrain-rmse:137.90489\tvalidation-rmse:139.66664\n",
      "[90]\ttrain-rmse:137.18688\tvalidation-rmse:139.10710\n",
      "[99]\ttrain-rmse:136.79354\tvalidation-rmse:138.89698\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "xg_reg_base = xgb.train(\n",
    "  params= {'objective': 'reg:squarederror', 'tree_method': 'hist'},\n",
    "  dtrain=xg_train_data,\n",
    "  num_boost_round=100,\n",
    "  evals=xg_eval_set,\n",
    "  verbose_eval=10,\n",
    "  early_stopping_rounds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550d1211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xg_reg_base</td>\n",
       "      <td>70.024869</td>\n",
       "      <td>138.896979</td>\n",
       "      <td>0.249842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model        MAE        RMSE        R²\n",
       "0  xg_reg_base  70.024869  138.896979  0.249842"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_base.predict(xg_val_data)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0a5e7",
   "metadata": {},
   "source": [
    "**MAE**<br>\n",
    "On average, the predictions are off by 70 seconds, which is reasonable, knowing that [STM](https://www.stm.info/en/info/networks/bus-network-and-schedules-enlightened) considers a bus arriving 3 minutes after the planned schedule as being on time.\n",
    "\n",
    "**RMSE**<br>\n",
    "The higher RMSE compared to MAE suggests that there are some significant prediction errors that influence the overall error metric.\n",
    "\n",
    "**R²**<br>\n",
    "The model explains 24.98% of the variance, which is not good but understandable because of how random transit delays can be (bad weather, vehicle breakdown, accidents, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960dd759",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3edbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression datasets\n",
    "lgb_train_data = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_val_data = lgb.Dataset(X_val, label=y_val, reference=lgb_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32750cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056318 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 891\n",
      "[LightGBM] [Info] Number of data points in the train set: 900000, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 52.317412\n",
      "Training until validation scores don't improve for 3 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[100]\tvalid_0's rmse: 145.377\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "lgb_reg_base = lgb.train(\n",
    "    params={\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': -1\n",
    "    },\n",
    "    train_set=lgb_train_data,\n",
    "    valid_sets=[lgb_val_data],\n",
    "    num_boost_round=100,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "673a86d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xg_reg_base</td>\n",
       "      <td>70.024869</td>\n",
       "      <td>138.896979</td>\n",
       "      <td>0.249842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lgb_reg_base</td>\n",
       "      <td>73.059300</td>\n",
       "      <td>145.376557</td>\n",
       "      <td>0.178219</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model        MAE        RMSE        R²\n",
       "0   xg_reg_base  70.024869  138.896979  0.249842\n",
       "1  lgb_reg_base  73.059300  145.376557  0.178219"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_pred = lgb_reg_base.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'lgb_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f15c1",
   "metadata": {},
   "source": [
    "The LightGBM model performs worse than XGBoost, especially in terms of R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a61f6b",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4b34ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 160.2333777\ttest: 159.4901464\tbest: 159.4901464 (0)\ttotal: 195ms\tremaining: 19.3s\n",
      "10:\tlearn: 154.7531486\ttest: 153.9896349\tbest: 153.9896349 (10)\ttotal: 1.85s\tremaining: 15s\n",
      "20:\tlearn: 152.2855611\ttest: 151.5800487\tbest: 151.5800487 (20)\ttotal: 3.22s\tremaining: 12.1s\n",
      "30:\tlearn: 150.7845821\ttest: 150.1601370\tbest: 150.1601370 (30)\ttotal: 4.88s\tremaining: 10.9s\n",
      "40:\tlearn: 149.8148871\ttest: 149.2456224\tbest: 149.2456224 (40)\ttotal: 6.33s\tremaining: 9.1s\n",
      "50:\tlearn: 148.8422416\ttest: 148.3812888\tbest: 148.3812888 (50)\ttotal: 7.79s\tremaining: 7.48s\n",
      "60:\tlearn: 148.1890750\ttest: 147.8152134\tbest: 147.8152134 (60)\ttotal: 9.29s\tremaining: 5.94s\n",
      "70:\tlearn: 147.4223265\ttest: 147.1410785\tbest: 147.1410785 (70)\ttotal: 10.8s\tremaining: 4.41s\n",
      "80:\tlearn: 146.8320243\ttest: 146.6404622\tbest: 146.6404622 (80)\ttotal: 12.3s\tremaining: 2.9s\n",
      "90:\tlearn: 146.3030198\ttest: 146.1582510\tbest: 146.1582510 (90)\ttotal: 13.9s\tremaining: 1.38s\n",
      "99:\tlearn: 145.8487742\ttest: 145.7830723\tbest: 145.7830723 (99)\ttotal: 15.5s\tremaining: 0us\n",
      "\n",
      "bestTest = 145.7830723\n",
      "bestIteration = 99\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x11eab1e80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "cat_reg_base = CatBoostRegressor(\n",
    "    iterations=100,\n",
    "    learning_rate=0.05,\n",
    "    depth=10,\n",
    "    random_seed=42,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "cat_reg_base.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c87e3b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xg_reg_base</td>\n",
       "      <td>70.024869</td>\n",
       "      <td>138.896979</td>\n",
       "      <td>0.249842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lgb_reg_base</td>\n",
       "      <td>73.059300</td>\n",
       "      <td>145.376557</td>\n",
       "      <td>0.178219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cat_reg_base</td>\n",
       "      <td>72.920011</td>\n",
       "      <td>145.783072</td>\n",
       "      <td>0.173617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model        MAE        RMSE        R²\n",
       "0   xg_reg_base  70.024869  138.896979  0.249842\n",
       "1  lgb_reg_base  73.059300  145.376557  0.178219\n",
       "2  cat_reg_base  72.920011  145.783072  0.173617"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_pred = cat_reg_base.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'cat_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e751ec",
   "metadata": {},
   "source": [
    "CatBoost performs almost like LightGBM. So far, XGBoost seems to capture a bit more of the underlying patterns than the two other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4284c",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd661f2f",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40747e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 50 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'alpha': [0, 1, 2, 5], # L1 regularization\n",
    "    'lambda': [0, 1, 2, 5] # L2 regularization\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2682ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "xg_reg_tuned = random_search.best_estimator_\n",
    "xg_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ca1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_tuned.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_tuned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b84a0",
   "metadata": {},
   "source": [
    "There's a significant improvement from the base XGBoost model and it's the best performing model so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688cf44e",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "  'n_estimators': [100, 200, 300, 400, 500, 600],\n",
    "  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  'max_depth': [5, 10, 15],\n",
    "  'num_leaves': [20, 31, 40],\n",
    "  'min_child_samples': [10, 20, 30],\n",
    "  'subsample': [0.8, 1.0],\n",
    "  'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=2, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "lgb_reg_tuned = random_search.best_estimator_\n",
    "lgb_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = lgb_reg_tuned.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'lgb_reg_tuned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf4eee",
   "metadata": {},
   "source": [
    "The performance is very similar to the previous tuned model. The MAE is slightly worse but the RMSE and the R-squared are slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d5086",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "  'iterations': [100, 200, 300, 400, 500, 600],\n",
    "  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  'depth': [6, 8, 10],\n",
    "  'l2_leaf_reg': [1, 3, 5],\n",
    "  'border_count': [32, 64, 128],\n",
    "  'bagging_temperature': [0, 1, 5],\n",
    "}\n",
    "\n",
    "cat_model = CatBoostRegressor(verbose=0, random_seed=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=cat_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=2,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "cat_reg_tuned = random_search.best_estimator_\n",
    "cat_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = cat_reg_tuned.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'cat_reg_tuned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a90fcf",
   "metadata": {},
   "source": [
    "After tuning, XGBoost is the best performing model. It also has the fastest fitting time. This is the model that will be used for the rest of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebbac9",
   "metadata": {},
   "source": [
    "## Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0854867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "best_model = xg_reg_tuned\n",
    "y_pred = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180dd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(y_true, y_pred, model_name:str) -> None:\n",
    "\tfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n",
    "\n",
    "\t# Predicted vs. actual values\n",
    "\tax1.scatter(x=y_pred, y=y_true)\n",
    "\tax1.set_title('Predicted vs. Actual values')\n",
    "\tax1.set_xlabel('Predicted Delay (seconds)')\n",
    "\tax1.set_ylabel('Actual Delay (seconds)')\n",
    "\tax1.grid(True)\n",
    "\n",
    "\t# Residuals\n",
    "\tresiduals = y_true - y_pred\n",
    "\tax2.scatter(x=y_pred, y=residuals)\n",
    "\tax2.set_title('Residual Plot')\n",
    "\tax2.set_xlabel('Predicted Delay (seconds)')\n",
    "\tax2.set_ylabel('Residuals (seconds)')\n",
    "\tax2.axhline(0, linestyle='--', color='orange')\n",
    "\tax2.grid(True)\n",
    "\n",
    "\tfig.suptitle('Residual Analysis', fontsize=18)\n",
    "\tfig.tight_layout()\n",
    "\tfig.savefig(f'../images/residual_analysis_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plot_residuals(y_val, y_pred, 'xg_reg_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52270f20",
   "metadata": {},
   "source": [
    "**Predicted vs. Actual Plot**\n",
    "\n",
    "There's a dense cluster around 0 for both predicted and actual values, indicating many predictions and centered near 0. However, there is substantial spread both above and below the diagonal line, which suggests underprediction and overprediction. There are clear outliers that are far from the main cluster.\n",
    "\n",
    "\n",
    "**Residual Plot**\n",
    "\n",
    "The residuals show a visible funnel shapes, which indicates a systematic error in prediction. The spread of residuals increases as the predicted delay increases. This is a sign of heteroscedasticity (the variance of errors is not constant across all predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5448a",
   "metadata": {},
   "source": [
    "## Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 most important features\n",
    "importances = best_model.get_booster().get_score(importance_type='weight')\n",
    "importances_df = pd.DataFrame.from_dict(importances, orient='index').rename(columns={0: 'importance'}).reset_index().rename(columns={'index': 'feature'})\n",
    "importances_df.sort_values('importance', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ad8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importance\n",
    "ax = xgb.plot_importance(best_model, importance_type='weight')\n",
    "ax.figure.tight_layout()\n",
    "ax.figure.savefig('../images/feature_importances_xg_reg_tuned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136884d",
   "metadata": {},
   "source": [
    "**Most Important Features:**\n",
    "- `exp_trip_duration` This is the most important feature in the model. It seems like the expected trip duration is highly predictive of the actual delay. This makes sense as longer expected trips are more prone to disruptions and variations.\n",
    "- `hist_avg_delay` Historical average delay is the second most important predictor. This aligns well with time series predictability since past delays often indicate patterns or bottlenecks that repeat over time.\n",
    "- `stop_distance` Long gaps might lead to longer travel times or greater variability.\n",
    "- `stop_location_group` This is also highly influential. Grouping the stops by location might be capturing specific problematic areas or geographic patterns that contribute to delays (e.g., heavy traffic zones, construction areas, major intersections).\n",
    "- `temperature_2m`, `wind_direction_10m`, `wind_speed_10m` Weather conditions do play a role, but not as heavily as trip-related features. The fact that wind and temperature are relatively impactful suggests weather variability might affect delays more than just precipitation alone.\n",
    "\n",
    "**Least Important Features:**\n",
    "- `is_peak_hour` This is surprisingly less impactful than expected. It suggests that perhaps peak hours are not as unpredictable as other features.\n",
    "- `frequency_very_frequent` The bus frequency is contributing to the prediction. More frequent buses might be less susceptible to delays since missed connections or unexpected traffic issues don't accumulate as much.\n",
    "-  `time_of_day_evening`, `time_of_day_morning`, `time_of_day_night` Evening seems to be a bit more influential than morning or night, which could indicate evening rush hour impacts.\n",
    "- `wheelchair_boarding` Very low importance, indicating it has minimal influence on delays.\n",
    "- `schedule_relationship_Scheduled` This has almost no impact, which might indicate that deviations from scheduled times are not systematically captured by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54f573",
   "metadata": {},
   "source": [
    "## SHAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_plot(shap_values, X_true, model_name:str, barplot:bool=True) -> None:\n",
    "\tif barplot:\n",
    "\t\tshap.summary_plot(shap_values, X_true, plot_type='bar', show=False)\n",
    "\t\tplt.title('SHAP Summary Barplot')\n",
    "\t\tplot_type = 'barplot' \n",
    "\telse: # beeswarm\n",
    "\t\tshap.summary_plot(shap_values, X_true, show=False)\n",
    "\t\tplt.title('SHAP Summary Beeswarm Plot')\n",
    "\t\tplot_type = 'beeswarm_plot' \n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'../images/shap_{plot_type}_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_single_pred(X_true, explainer, shap_values, model_name:str) -> None:\n",
    "\trandom.seed(42)\n",
    "\tindex = random.randrange(len(X_true))\n",
    "\tshap.force_plot(\n",
    "\t\texplainer.expected_value,\n",
    "\t\tshap_values[index, :],\n",
    "\t\tX_true.iloc[index, :],\n",
    "\t\tfigsize=(15, 4),\n",
    "\t\tcontribution_threshold=0.1,\n",
    "\t\tmatplotlib=True,\n",
    "\t\tshow=False)\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'../images/shap_force_plot_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP\n",
    "sample_size = 10000 # sample validation set to prevent memory overload\n",
    "X_val_sample = X_val.sample(n=sample_size, random_state=42) \n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_val_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary barplot\n",
    "shap_plot(shap_values, X_val_sample, 'xg_reg_tuned', barplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afe1ce",
   "metadata": {},
   "source": [
    "**Comparison with XGBoost feature importances**\n",
    "\n",
    "- `hist_avg_delay` is a much stronger predictor in SHAP analysis than XGBoost's internal measure. It’s clearly impactful across different delays.\n",
    "frequency_very_rare is hidden in XGBoost’s view, but SHAP recognizes it as influential — possibly due to rare scheduling patterns causing large delays.\n",
    "- `stop_distance` is potentially overvalued by XGBoost. SHAP indicates it might not be as impactful.\n",
    "- SHAP considers `time_of_day_evening` to be more important. SHAP probably picked up interaction that XGBoost ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary beeswarm plot\n",
    "shap_plot(shap_values, X_val_sample, 'xg_reg_tuned', barplot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf14bb",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "- `hist_avg_delay` and `exp_trip_duration` are the top features:\n",
    "\t- High values of hist_avg_delay (in red) push predictions higher.\n",
    "\t- For `exp_trip_duration`, high values both increase and decrease the delay prediction, indicating complex interactions.\n",
    "- Weather Variables:\n",
    "\t- `temperature_2m`, `wind_speed_10m`, and `wind_direction_10m` also affect predictions. For instance, higher wind speeds push predictions slightly upwards, which makes sense given that weather disturbances can slow down traffic.\n",
    "- Time of Day:\n",
    "\t- Evening seems to affect delay more than morning or night, aligning with typical rush hour traffic.\n",
    "\n",
    "**Insight:**\n",
    "\n",
    "The high influence of `hist_avg_delay` confirms that delay is highly dependent on past performance. This could be useful for forecasting in specific segments or optimizing bus routes during peak times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot a single prediction\n",
    "shap_single_pred(X_val_sample, explainer, shap_values, 'xg_reg_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608349d0",
   "metadata": {},
   "source": [
    "This plot is a breakdown of the specific prediction (`34.97`) for one instance. For clarity, only the features with a SHAP score of at least 1 are displayed.\n",
    "\n",
    "- Features that increase the prediction (red):\n",
    "\t- `time_of_day_evening`: Being in the evening strongly increases the delay, especially at peak times.\n",
    "\t- `frequency_rare`, `frequency_very_rare`: If the bus service is rare, it also increases the expected delay.\n",
    "- Features that decrease the prediction (blue):\n",
    "\t- `hist_avg_delay`: A value of `34.05` decreases the day.\n",
    "\t- `stop_location_group`: A value of `5.0` (East from downtown Montreal) also reduces the delay, which is expected, depending of the time of the day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac48272",
   "metadata": {},
   "source": [
    "## Feature Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af2650",
   "metadata": {},
   "source": [
    "XGBoost's internal metrics (Gain) are purely split-based and don’t account for interactions or marginal effects. SHAP, on the other hand, shows the actual contribution to predictions — highlighting that some of these splits might be less impactful than XGBoost suggests. Therefore, the feature elimination will be based on the SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717743e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_abs_mean = np.abs(shap_values).mean(axis=0)\n",
    "shap_df = pd.DataFrame({\n",
    "  'feature': X.columns,\n",
    "  'mean_abs_shap': shap_abs_mean\n",
    "})\n",
    "shap_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a90f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify low impact features (below 1)\n",
    "low_impact_features = shap_df[shap_df['mean_abs_shap'] < 1]\n",
    "\n",
    "print('Features with low SHAP impact to remove:\\n')\n",
    "print(low_impact_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bf5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features\n",
    "features_to_drop = low_impact_features['feature'].tolist()\n",
    "\n",
    "X_pruned = X.drop(features_to_drop, axis=1)\n",
    "X_train_pruned = X_train.drop(features_to_drop, axis=1)\n",
    "X_val_pruned = X_val.drop(features_to_drop, axis=1)\n",
    "X_test_pruned = X_test.drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec1d96",
   "metadata": {},
   "source": [
    "## Retrain Model with Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new regression matrices\n",
    "xg_train_data = xgb.DMatrix(X_train_pruned, y_train, enable_categorical=False)\n",
    "xg_val_data = xgb.DMatrix(X_val_pruned, y_val, enable_categorical=False)\n",
    "xg_test_data = xgb.DMatrix(X_test_pruned, y_test, enable_categorical=False)\n",
    "xg_eval_set = [(xg_train_data, 'train'), (xg_val_data, 'validation')]\n",
    "xg_test_set = [(xg_train_data, 'train'), (xg_test_data, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain model\n",
    "xg_reg_pruned = xgb.train(\n",
    "  params= {\n",
    "    'objective':'reg:squarederror', \n",
    "  \t'tree_method': 'hist',\n",
    "    'max_depth': xg_best_params['max_depth'],\n",
    "    'learning_rate': xg_best_params['learning_rate'],\n",
    "    'subsample': xg_best_params['subsample'],\n",
    "    'colsample_bytree': xg_best_params['colsample_bytree']\n",
    "  },\n",
    "  dtrain=xg_train_data,\n",
    "  num_boost_round=xg_best_params['n_estimators'],\n",
    "  evals=xg_eval_set,\n",
    "  verbose_eval=50,\n",
    "  early_stopping_rounds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_pruned.predict(xg_val_data)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_pruned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1dea0",
   "metadata": {},
   "source": [
    "The performance got worse with feature pruning. The best model of all is the tuned XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a0186",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb04b33",
   "metadata": {},
   "source": [
    "### Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "xg_train_data = xgb.DMatrix(X_train, y_train, enable_categorical=False)\n",
    "xg_val_data = xgb.DMatrix(X_val, y_val, enable_categorical=False)\n",
    "xg_test_data = xgb.DMatrix(X_test, y_test, enable_categorical=False)\n",
    "xg_eval_set = [(xg_train_data, 'train'), (xg_val_data, 'validation')]\n",
    "xg_test_set = [(xg_train_data, 'train'), (xg_test_data, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c906fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with more boosting rounds\n",
    "xg_reg_final = xgb.train(\n",
    "  params= {\n",
    "    'objective':'reg:squarederror', \n",
    "  \t'tree_method': 'hist',\n",
    "    'max_depth': xg_best_params['max_depth'],\n",
    "    'learning_rate': xg_best_params['learning_rate'],\n",
    "    'subsample': xg_best_params['subsample'],\n",
    "    'colsample_bytree': xg_best_params['colsample_bytree']\n",
    "  },\n",
    "  dtrain=xg_train_data,\n",
    "  num_boost_round=10000,\n",
    "  evals=xg_eval_set,\n",
    "  verbose_eval=50,\n",
    "  early_stopping_rounds=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ac4ed",
   "metadata": {},
   "source": [
    "### Evaluate with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = xg_reg_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a96346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = final_model.predict(xg_test_data)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_test, 'xg_reg_final')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353241a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plot_residuals(y_test, y_pred, 'xg_reg_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0820b",
   "metadata": {},
   "source": [
    "### SHAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7fbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP\n",
    "sample_size = 10000 # sample validation set to prevent memory overload\n",
    "X_test_sample = X_test.sample(n=sample_size, random_state=42) \n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary barplot\n",
    "shap_plot(shap_values, X_test_sample, 'cat_reg_final', barplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325648ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary beeswarm plot\n",
    "shap_plot(shap_values, X_test_sample, 'cat_reg_final', barplot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot a single prediction\n",
    "shap_single_pred(X_test_sample, explainer, shap_values, 'cat_reg_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e5c92",
   "metadata": {},
   "source": [
    "### Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display features\n",
    "best_features = X.columns.tolist()\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "test_input = {\n",
    "  \t'cloud_cover': [0],\n",
    "\t'exp_trip_duration': [3600],\n",
    "  \t'frequency_normal': [1],\n",
    "\t\n",
    "\t\n",
    "\t'relative_humidity_2m': [60],\n",
    "\t'wind_direction_10m': [140],\n",
    "\t'precipitation': [0],\n",
    "\t'time_of_day_morning': [0],\n",
    "\t'hist_avg_delay': [300],\n",
    "\t'route_direction_South': [0],\n",
    "\t'wind_speed_10m': [10],\n",
    "\t\n",
    "\t'time_of_day_evening': [0],\n",
    "\t'stop_location_group': [2],\n",
    "\t'is_peak_hour': [1],\n",
    "\t'trip_phase_middle': [0],\n",
    "\t'frequency_very_rare': [0],\n",
    "\t'route_direction_North': [0],\n",
    "\t'route_direction_West': [1],\n",
    "\t'frequency_rare': [0],\n",
    "\t'temperature_2m': [24.3],\n",
    "\t'stop_distance': [400],\n",
    "\t\n",
    "\t'trip_phase_start': [0]\n",
    "}\n",
    "\n",
    "x_test = pd.DataFrame(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb883fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict delay\n",
    "prediction = final_model.predict(x_test)\n",
    "print(f'Predicted delay: {prediction[0]:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8a024",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a144f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, hyperparameters and predictors\n",
    "joblib.dump(final_model, '../models/regression_model.pkl')\n",
    "joblib.dump(xg_best_params, '../models/best_hyperparams.pkl')\n",
    "joblib.dump(best_features, '../models/best_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e39b53",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
