{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6efc2fa1",
   "metadata": {},
   "source": [
    "# STM Transit Delay Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48c78c",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4739c1",
   "metadata": {},
   "source": [
    "This notebook explores tree-based machine learning models in order to find the one that predicts STM transit delays with the best accuracy. The featured models are XGBoost, LightGBM and CatBoost, because they are more suitable for large datasets with mixed data and high cardinality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d218f9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dded95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import sys\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3edf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom code\n",
    "sys.path.insert(0, '..')\n",
    "from src.helper_functions import get_top_abs_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d19b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('../data/preprocessed.parquet')\n",
    "print(f'Shape of dataset: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc11b74",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51490206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target variable\n",
    "X = df.drop('delay', axis=1)\n",
    "y = df['delay']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4a098",
   "metadata": {},
   "source": [
    "The 3 models can run multiple iterations with a training and validation set. Therefore, a hold-out set will be kept to evaluate the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split (60-20-20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "del X_temp\n",
    "del y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f9998",
   "metadata": {},
   "source": [
    "**Scaling**\n",
    "\n",
    "Since only tree-based models are explored in this project, scaling is not needed because the models are not sensitive to the absolute scale or distribution of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a69b3",
   "metadata": {},
   "source": [
    "## Fit Base Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce7b6b",
   "metadata": {},
   "source": [
    "All models allow to setup a number of rounds and early stopping. To start, all models will run 100 rounds with an early stopping of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to track metrics\n",
    "metrics_df = pd.DataFrame(columns=['model', 'MAE', 'RMSE', 'R²'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reg_metrics(metrics_df:pd.DataFrame, y_pred:pd.Series, y_true:pd.Series, model_name:str) -> pd.DataFrame:\n",
    "\tmae = mean_absolute_error(y_true, y_pred)\n",
    "\trmse = root_mean_squared_error(y_true, y_pred)\n",
    "\tr2 = r2_score(y_true, y_pred)\n",
    "\n",
    "\tmetrics_df.loc[len(metrics_df)] = [model_name, mae, rmse, r2]\n",
    "\treturn metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f3056",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "xg_train_data = xgb.DMatrix(X_train, y_train, enable_categorical=False)\n",
    "xg_val_data = xgb.DMatrix(X_val, y_val, enable_categorical=False)\n",
    "xg_eval_set = [(xg_train_data, 'train'), (xg_val_data, 'validation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "xg_reg_base = xgb.train(\n",
    "  params= {'objective': 'reg:squarederror', 'tree_method': 'hist'},\n",
    "  dtrain=xg_train_data,\n",
    "  num_boost_round=100,\n",
    "  evals=xg_eval_set,\n",
    "  verbose_eval=10,\n",
    "  early_stopping_rounds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_base.predict(xg_val_data)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0a5e7",
   "metadata": {},
   "source": [
    "**MAE**<br>\n",
    "On average, the predictions are off by 69 seconds, which is reasonable, knowing that [STM](https://www.stm.info/en/info/networks/bus-network-and-schedules-enlightened) considers a bus arriving 3 minutes after the planned schedule as being on time.\n",
    "\n",
    "**RMSE**<br>\n",
    "The higher RMSE compared to MAE suggests that there are some significant prediction errors that influence the overall error metric.\n",
    "\n",
    "**R²**<br>\n",
    "The model explains 27.9% of the variance, which is not good but understandable because of how random transit delays can be (bad weather, vehicle breakdown, accidents, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960dd759",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3edbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression datasets\n",
    "lgb_train_data = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_val_data = lgb.Dataset(X_val, label=y_val, reference=lgb_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32750cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "lgb_reg_base = lgb.train(\n",
    "    params={\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': -1\n",
    "    },\n",
    "    train_set=lgb_train_data,\n",
    "    valid_sets=[lgb_val_data],\n",
    "    num_boost_round=100,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = lgb_reg_base.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'lgb_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f15c1",
   "metadata": {},
   "source": [
    "The LightGBM model performs worse than XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a61f6b",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b34ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "cat_reg_base = CatBoostRegressor(\n",
    "    iterations=100,\n",
    "    learning_rate=0.05,\n",
    "    depth=10,\n",
    "    random_seed=42,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "cat_reg_base.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = cat_reg_base.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'cat_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e751ec",
   "metadata": {},
   "source": [
    "CatBoost performs almost like LightGBM, but has a longer fitting time. XGBoost seems to capture more of the underlying patterns than the two other models. This is the model that will be used for analysis and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c152f51",
   "metadata": {},
   "source": [
    "## Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6aedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(y_true, y_pred, model_name:str) -> None:\n",
    "\tfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n",
    "\n",
    "\t# Predicted vs. actual values\n",
    "\tax1.scatter(x=y_pred, y=y_true)\n",
    "\tax1.set_title('Predicted vs. Actual values')\n",
    "\tax1.set_xlabel('Predicted Delay (seconds)')\n",
    "\tax1.set_ylabel('Actual Delay (seconds)')\n",
    "\tax1.grid(True)\n",
    "\n",
    "\t# Residuals\n",
    "\tresiduals = y_true - y_pred\n",
    "\tax2.scatter(x=y_pred, y=residuals)\n",
    "\tax2.set_title('Residual Plot')\n",
    "\tax2.set_xlabel('Predicted Delay (seconds)')\n",
    "\tax2.set_ylabel('Residuals (seconds)')\n",
    "\tax2.axhline(0, linestyle='--', color='orange')\n",
    "\tax2.grid(True)\n",
    "\n",
    "\tfig.suptitle('Residual Analysis', fontsize=18)\n",
    "\tfig.tight_layout()\n",
    "\tfig.savefig(f'../images/residual_analysis_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plot_residuals(y_val, y_pred, 'xg_reg_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9334e",
   "metadata": {},
   "source": [
    "**Predicted vs. Actual Plot**\n",
    "\n",
    "There's a dense cluster around 0 for both predicted and actual values, indicating many predictions and centered near 0. However, there is substantial spread both above and below the diagonal line, which suggests underprediction and overprediction. There are clear outliers that are far from the main cluster.\n",
    "\n",
    "\n",
    "**Residual Plot**\n",
    "\n",
    "The residuals show a visible funnel shapes, which indicates a systematic error in prediction. The spread of residuals increases as the predicted delay increases. This is a sign of heteroscedasticity (the variance of errors is not constant across all predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4284c",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40747e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'alpha': [0, 1, 2, 3, 4, 5],\n",
    "    'lambda': [0, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_estimators=100)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    n_iter=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2682ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "xg_reg_tuned = random_search.best_estimator_\n",
    "xg_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ca1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_tuned.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_tuned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580dd59",
   "metadata": {},
   "source": [
    "The performance didn't improve that much from the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5448a",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 most important features\n",
    "importances = xg_reg_tuned.get_booster().get_score()\n",
    "importances_df = pd.DataFrame.from_dict(importances, orient='index') \\\n",
    "\t.rename(columns={0: 'importance'}).reset_index() \\\n",
    "\t.rename(columns={'index': 'feature'})\n",
    "importances_df.sort_values('importance', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ad8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances\n",
    "ax = xgb.plot_importance(xg_reg_tuned)\n",
    "ax.figure.tight_layout()\n",
    "ax.figure.savefig('../images/feature_importances_xg_reg_tuned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136884d",
   "metadata": {},
   "source": [
    "**Most Important Features:**\n",
    "- `exp_trip_duration` This is the most important feature in the model. It seems like the expected trip duration is highly predictive of the actual delay. This makes sense as longer expected trips are more prone to disruptions and variations.\n",
    "- `hist_avg_delay` Historical average delay is the second most important predictor. This aligns well with time series predictability since past delays often indicate patterns or bottlenecks that repeat over time.\n",
    "- `route_bearing` The direction of a vehicle might indicate if it's in the direction of traffic or not.\n",
    "- `arrivals_per_hour` The bus frequency is contributing to the prediction. Less frequent buses might be more susceptible to delays since missed connections or unexpected traffic issues tend to accumulate.\n",
    "- `trip_progress` Delays accumulate when the vehicle is further along the trip.\n",
    "\n",
    "\n",
    "**Least Important Features:**\n",
    "- `schedule_relationship_Scheduled` This has low impact, which might indicate that deviations from scheduled times are not systematically captured by the model.\n",
    "- `is_peak_hour` This is surprisingly less impactful than expected. It suggests that perhaps peak hours are not as unpredictable as other features.\n",
    "- `time_of_day_morning`, `time_of_day_night` Evening seems to be a bit more influential than morning or night, which could indicate evening rush hour impacts.\n",
    "- `wheelchair_boarding` Very low importance, indicating it has minimal influence on delays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54f573",
   "metadata": {},
   "source": [
    "## SHAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_plot(shap_values, X_true, model_name:str, barplot:bool=True) -> None:\n",
    "\tif barplot:\n",
    "\t\tshap.summary_plot(shap_values, X_true, plot_type='bar', show=False)\n",
    "\t\tplt.title('SHAP Summary Barplot')\n",
    "\t\tplot_type = 'barplot' \n",
    "\telse: # beeswarm\n",
    "\t\tshap.summary_plot(shap_values, X_true, show=False)\n",
    "\t\tplt.title('SHAP Summary Beeswarm Plot')\n",
    "\t\tplot_type = 'beeswarm_plot' \n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'../images/shap_{plot_type}_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_single_pred(X_true, explainer, shap_values, model_name:str) -> None:\n",
    "\trandom.seed(10)\n",
    "\tindex = random.randrange(len(X_true))\n",
    "\tshap.force_plot(\n",
    "\t\texplainer.expected_value,\n",
    "\t\tshap_values[index, :],\n",
    "\t\tX_true.iloc[index, :],\n",
    "\t\tfigsize=(30, 4),\n",
    "\t\tmatplotlib=True,\n",
    "\t\tshow=False)\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'../images/shap_force_plot_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP\n",
    "X_val_sample = X_val.sample(25000, random_state=42) # sample validation set to prevent memory overload\n",
    "explainer = shap.TreeExplainer(xg_reg_tuned)\n",
    "shap_values = explainer.shap_values(X_val_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary barplot\n",
    "shap_plot(shap_values, X_val_sample, 'xg_reg_tuned', barplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afe1ce",
   "metadata": {},
   "source": [
    "**Comparison with XGBoost feature importances**\n",
    "\n",
    "- Interestingly, `hist_avg_delay` comes out as the most impactful in this plot, while it was second in the XGBoost importance plot.\n",
    "- `arrivals_per_hour` is elevated to the second position, while in the default importance, it is in fourth place.\n",
    "- This suggests that in terms of predictive influence, `hist_avg_delay` and `arrivals_per_hour` are actually more significant than what the XGBoost default metric captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary beeswarm plot\n",
    "shap_plot(shap_values, X_val_sample, 'xg_reg_tuned', barplot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf14bb",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "- High `hist_avg_delay` (red) tends to push predictions higher, and low values (blue) push it lower. The high influence of `hist_avg_delay` confirms that delay is highly dependent on past performance. This could be useful for forecasting in specific segments or optimizing bus routes during peak times.\n",
    "- Some features like `route_bearing` and `exp_avg_delay` have either a positive or negative infuence, suggesting there's a more complex feature interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot a single prediction\n",
    "shap_single_pred(X_val_sample, explainer, shap_values, 'xg_reg_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608349d0",
   "metadata": {},
   "source": [
    "This plot is a breakdown of the specific prediction (`28.31`) for one instance.\n",
    "\n",
    "- Features that increase the prediction (red):\n",
    "\t- `exp_trip_duration`: A expected trip duration of `2280.0` (38 minutes).\n",
    "\t- `stop_distance`: The distance from the previous stop being `155.94` meters.\n",
    "\t- `wind_speed_10m`: A wind speed of `5.8` kilometers per hour.\n",
    "\n",
    "- Features that decrease the prediction (blue):\n",
    "\t- `hist_avg_delay`: The historical average delay being `-14.2` seconds.\n",
    "\t- `arrivals_per_hour`: A bus that arrives `2.0` times per hour.\n",
    "\t- `route_bearing`: A value of `115.15` (South-East).\n",
    "\t- `trip_progress`: A bus at the middle of its trip (`0.38`).\n",
    "\t- `temperature_2m`: The temperature being `16.0` degrees Celsius.\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac48272",
   "metadata": {},
   "source": [
    "## Feature Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c13bde",
   "metadata": {},
   "source": [
    "### Add feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8fa3b",
   "metadata": {},
   "source": [
    "Some features were surprisingly low impact and the SHAP plots suggest there might be interactions between some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate second degree polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_val_poly = poly.transform(X_val)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "X_poly = pd.DataFrame(X_poly, columns=poly.get_feature_names_out())\n",
    "X_train_poly = pd.DataFrame(X_train_poly, columns=poly.get_feature_names_out())\n",
    "X_val_poly = pd.DataFrame(X_val_poly, columns=poly.get_feature_names_out())\n",
    "X_test_poly = pd.DataFrame(X_test_poly, columns=poly.get_feature_names_out())\n",
    "\n",
    "X_train_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit model\n",
    "xg_reg_poly = xgb.XGBRegressor(\n",
    "  \tobjective='reg:squarederror',\n",
    "  \trandom_state=42,\n",
    "  \tn_estimators=100,\n",
    "    max_depth=xg_best_params['max_depth'],\n",
    "    learning_rate=xg_best_params['learning_rate'],\n",
    "    subsample=xg_best_params['subsample'],\n",
    "    colsample_bytree=xg_best_params['colsample_bytree'],\n",
    "    alpha=xg_best_params['alpha'],\n",
    "    reg_lambda=xg_best_params['lambda']\n",
    ")\n",
    "\n",
    "xg_reg_poly.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfcad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_poly.predict(X_val_poly)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_poly')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c23ac5",
   "metadata": {},
   "source": [
    "There is a slight improvement from the model without the interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba73384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Mutual Information to rank feature importance\n",
    "mi_scores = mutual_info_regression(X_poly, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "feature_names = poly.get_feature_names_out(X.columns)\n",
    "mi_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Mutual Information Score': mi_scores\n",
    "}).sort_values(by='Mutual Information Score', ascending=False)\n",
    "mi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75dc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the top 10 interactions\n",
    "top_10_features = mi_df.head(20)['Feature'].tolist()\n",
    "X_top_10 = X_poly[top_10_features]\n",
    "X_top_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heatmap of the correlations between these top features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X_top_10.corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Heatmap of Top 10 Interaction Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0adcb9",
   "metadata": {},
   "source": [
    "### Keep best interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb497bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features with a score greater than 0.1\n",
    "best_features = mi_df[mi_df['Mutual Information Score'] > 0.1]['Feature'].sort_values().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep best features\n",
    "X_best = X_poly[best_features]\n",
    "X_train_best = X_train_poly[best_features]\n",
    "X_val_best = X_val_poly[best_features]\n",
    "X_test_best = X_test_poly[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afa32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most correlated interactions\n",
    "correlations = get_top_abs_correlations(X_best)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdb657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features\n",
    "features_to_drop = list(set(correlations.index.get_level_values(level=1)))\n",
    "\n",
    "X_best = X_best.drop(features_to_drop, axis=1)\n",
    "X_train_best = X_train_best.drop(features_to_drop, axis=1)\n",
    "X_val_best = X_val_best.drop(features_to_drop, axis=1)\n",
    "X_test_best = X_test_best.drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec1d96",
   "metadata": {},
   "source": [
    "## Retrain Model with Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit model\n",
    "xg_reg_pruned = xgb.XGBRegressor(\n",
    "  \tobjective='reg:squarederror',\n",
    "  \trandom_state=42,\n",
    "  \tn_estimators=100,\n",
    "    max_depth=xg_best_params['max_depth'],\n",
    "    learning_rate=xg_best_params['learning_rate'],\n",
    "    subsample=xg_best_params['subsample'],\n",
    "    colsample_bytree=xg_best_params['colsample_bytree'],\n",
    "    alpha=xg_best_params['alpha'],\n",
    "    reg_lambda=xg_best_params['lambda']\n",
    ")\n",
    "\n",
    "xg_reg_pruned.fit(X_train_best, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_pruned.predict(X_val_best)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_pruned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1dea0",
   "metadata": {},
   "source": [
    "The performance got slighly better with less polynomial features, which is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84be67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform another randomized search\n",
    "param_dist = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'alpha': [0, 1, 2, 3, 4, 5],\n",
    "    'lambda': [0, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_estimators=100)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    n_iter=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_best, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018436e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "xg_best_model = random_search.best_estimator_\n",
    "xg_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a0186",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb04b33",
   "metadata": {},
   "source": [
    "### Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "xg_train_data = xgb.DMatrix(X_train_best, y_train, enable_categorical=False)\n",
    "xg_val_data = xgb.DMatrix(X_val_best, y_val, enable_categorical=False)\n",
    "xg_test_data = xgb.DMatrix(X_test_best, y_test, enable_categorical=False)\n",
    "xg_eval_set = [(xg_train_data, 'train'), (xg_val_data, 'validation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c906fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with more boosting rounds\n",
    "final_model = xgb.train(\n",
    "  params= {\n",
    "    'objective':'reg:squarederror', \n",
    "  \t'tree_method': 'hist',\n",
    "    'max_depth': xg_best_params['max_depth'],\n",
    "    'learning_rate': xg_best_params['learning_rate'],\n",
    "    'subsample': xg_best_params['subsample'],\n",
    "    'colsample_bytree': xg_best_params['colsample_bytree'],\n",
    "\t'alpha': xg_best_params['alpha'],\n",
    "    'lambda': xg_best_params['lambda'],\n",
    "  },\n",
    "  dtrain=xg_train_data,\n",
    "  num_boost_round=10000,\n",
    "  evals=xg_eval_set,\n",
    "  verbose_eval=50,\n",
    "  early_stopping_rounds=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ac4ed",
   "metadata": {},
   "source": [
    "### Evaluate with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a96346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = final_model.predict(xg_test_data)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_test, 'xg_reg_final')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353241a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plot_residuals(y_test, y_pred, 'xg_reg_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0820b",
   "metadata": {},
   "source": [
    "### SHAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7fbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP\n",
    "sample_size = 1000 # sample test set\n",
    "X_test_sample = X_test.sample(n=sample_size, random_state=42) \n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary barplot\n",
    "shap_plot(shap_values, X_test_sample, 'xg_reg_final', barplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325648ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary beeswarm plot\n",
    "shap_plot(shap_values, X_test_sample, 'xg_reg_final', barplot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot a single prediction\n",
    "shap_single_pred(X_test_sample, explainer, shap_values, 'xg_reg_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e5c92",
   "metadata": {},
   "source": [
    "### Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display features\n",
    "best_features = X_best.columns.tolist()\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "test_input = {\n",
    "#   \t'cloud_cover': [0],\n",
    "# \t'exp_trip_duration': [3600],\n",
    "#   \t'frequency_normal': [1],\n",
    "\t\n",
    "\t\n",
    "# \t'relative_humidity_2m': [60],\n",
    "# \t'wind_direction_10m': [140],\n",
    "# \t'precipitation': [0],\n",
    "# \t'time_of_day_morning': [0],\n",
    "# \t'hist_avg_delay': [300],\n",
    "# \t'route_direction_South': [0],\n",
    "# \t'wind_speed_10m': [10],\n",
    "\t\n",
    "# \t'time_of_day_evening': [0],\n",
    "# \t'stop_location_group': [2],\n",
    "# \t'is_peak_hour': [1],\n",
    "# \t'trip_phase_middle': [0],\n",
    "# \t'frequency_very_rare': [0],\n",
    "# \t'route_direction_North': [0],\n",
    "# \t'route_direction_West': [1],\n",
    "# \t'frequency_rare': [0],\n",
    "# \t'temperature_2m': [24.3],\n",
    "# \t'stop_distance': [400],\n",
    "\t\n",
    "# \t'trip_phase_start': [0]\n",
    "}\n",
    "\n",
    "x_test = pd.DataFrame(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb883fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict delay\n",
    "prediction = final_model.predict(x_test)\n",
    "print(f'Predicted delay: {prediction[0]:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8a024",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, hyperparameters and predictors\n",
    "joblib.dump(final_model, '../models/regression_model.pkl')\n",
    "joblib.dump(xg_best_params, '../models/best_hyperparams.pkl')\n",
    "joblib.dump(best_features, '../models/best_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e39b53",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
