{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6efc2fa1",
   "metadata": {},
   "source": [
    "# STM Transit Delay Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48c78c",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4739c1",
   "metadata": {},
   "source": [
    "This notebook explores tree-based machine learning models in order to find the one that predicts STM transit delays with the best accuracy. The featured models are XGBoost, LightGBM and CatBoost, because they are more suitable for large datasets with mixed data and high cardinality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d218f9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dded95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import sys\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3edf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom code\n",
    "sys.path.insert(0, '..')\n",
    "from src.helper_functions import get_top_abs_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d19b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (1500000, 21)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('../data/preprocessed.parquet')\n",
    "print(f'Shape of dataset: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc11b74",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51490206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target variable\n",
    "X = df.drop('delay', axis=1)\n",
    "y = df['delay']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4a098",
   "metadata": {},
   "source": [
    "The 3 models can run multiple iterations with a training and validation set. Therefore, a hold-out set will be kept to evaluate the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee2060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split (60-20-20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "del X_temp\n",
    "del y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f9998",
   "metadata": {},
   "source": [
    "**Scaling**\n",
    "\n",
    "Since only tree-based models are explored in this project, **scaling is not needed** because the models are not sensitive to the absolute scale or distribution of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a69b3",
   "metadata": {},
   "source": [
    "## Fit Base Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce7b6b",
   "metadata": {},
   "source": [
    "All models allow to setup a number of rounds and early stopping. To start, all models will run 100 rounds with an early stopping of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f96f66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to track metrics\n",
    "metrics_df = pd.DataFrame(columns=['model', 'MAE', 'RMSE', 'R²'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a14b6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reg_metrics(metrics_df:pd.DataFrame, y_pred:pd.Series, y_true:pd.Series, model_name:str) -> pd.DataFrame:\n",
    "\tmae = mean_absolute_error(y_true, y_pred)\n",
    "\trmse = root_mean_squared_error(y_true, y_pred)\n",
    "\tr2 = r2_score(y_true, y_pred)\n",
    "\n",
    "\tmetrics_df.loc[len(metrics_df)] = [model_name, mae, rmse, r2]\n",
    "\treturn metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f3056",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10d4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "xg_train_data = xgb.DMatrix(X_train, y_train, enable_categorical=False)\n",
    "xg_val_data = xgb.DMatrix(X_val, y_val, enable_categorical=False)\n",
    "xg_eval_set = [(xg_train_data, 'train'), (xg_val_data, 'validation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8303fcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:156.39455\tvalidation-rmse:155.63622\n",
      "[10]\ttrain-rmse:147.63804\tvalidation-rmse:147.64934\n",
      "[20]\ttrain-rmse:144.79119\tvalidation-rmse:145.32511\n",
      "[30]\ttrain-rmse:142.92251\tvalidation-rmse:143.65306\n",
      "[40]\ttrain-rmse:141.10499\tvalidation-rmse:142.15731\n",
      "[50]\ttrain-rmse:139.14415\tvalidation-rmse:140.41181\n",
      "[60]\ttrain-rmse:137.65209\tvalidation-rmse:139.10703\n",
      "[70]\ttrain-rmse:136.82802\tvalidation-rmse:138.48722\n",
      "[80]\ttrain-rmse:135.72298\tvalidation-rmse:137.51687\n",
      "[90]\ttrain-rmse:134.60838\tvalidation-rmse:136.60422\n",
      "[99]\ttrain-rmse:133.59100\tvalidation-rmse:135.83129\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "xg_reg_base = xgb.train(\n",
    "  params= {'objective': 'reg:squarederror', 'tree_method': 'hist'},\n",
    "  dtrain=xg_train_data,\n",
    "  num_boost_round=100,\n",
    "  evals=xg_eval_set,\n",
    "  verbose_eval=10,\n",
    "  early_stopping_rounds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "550d1211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xg_reg_base</td>\n",
       "      <td>69.204192</td>\n",
       "      <td>135.83129</td>\n",
       "      <td>0.282591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model        MAE       RMSE        R²\n",
       "0  xg_reg_base  69.204192  135.83129  0.282591"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_base.predict(xg_val_data)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0a5e7",
   "metadata": {},
   "source": [
    "**MAE**<br>\n",
    "On average, the predictions are off by 69 seconds, which is reasonable, knowing that [STM](https://www.stm.info/en/info/networks/bus-network-and-schedules-enlightened) considers a bus arriving 3 minutes after the planned schedule as being on time.\n",
    "\n",
    "**RMSE**<br>\n",
    "The higher RMSE compared to MAE suggests that there are some significant prediction errors that influence the overall error metric.\n",
    "\n",
    "**R²**<br>\n",
    "The model explains 28.26% of the variance, which is not good but understandable because of how random transit delays can be (bad weather, vehicle breakdown, accidents, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960dd759",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3edbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression datasets\n",
    "lgb_train_data = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_val_data = lgb.Dataset(X_val, label=y_val, reference=lgb_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32750cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "lgb_reg_base = lgb.train(\n",
    "    params={\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': -1\n",
    "    },\n",
    "    train_set=lgb_train_data,\n",
    "    valid_sets=[lgb_val_data],\n",
    "    num_boost_round=100,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = lgb_reg_base.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'lgb_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f15c1",
   "metadata": {},
   "source": [
    "The LightGBM model performs worse than XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a61f6b",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b34ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "cat_reg_base = CatBoostRegressor(\n",
    "    iterations=100,\n",
    "    learning_rate=0.05,\n",
    "    depth=10,\n",
    "    random_seed=42,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "cat_reg_base.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = cat_reg_base.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'cat_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e751ec",
   "metadata": {},
   "source": [
    "CatBoost performs almost like LightGBM, but has a longer fitting time. XGBoost seems to capture more of the underlying patterns than the two other models. This is the model that will be used for analysis and tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c152f51",
   "metadata": {},
   "source": [
    "## Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6aedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(y_true, y_pred, model_name:str) -> None:\n",
    "\tfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n",
    "\n",
    "\t# Predicted vs. actual values\n",
    "\tax1.scatter(x=y_pred, y=y_true)\n",
    "\tax1.set_title('Predicted vs. Actual values')\n",
    "\tax1.set_xlabel('Predicted Delay (seconds)')\n",
    "\tax1.set_ylabel('Actual Delay (seconds)')\n",
    "\tax1.grid(True)\n",
    "\n",
    "\t# Residuals\n",
    "\tresiduals = y_true - y_pred\n",
    "\tax2.scatter(x=y_pred, y=residuals)\n",
    "\tax2.set_title('Residual Plot')\n",
    "\tax2.set_xlabel('Predicted Delay (seconds)')\n",
    "\tax2.set_ylabel('Residuals (seconds)')\n",
    "\tax2.axhline(0, linestyle='--', color='orange')\n",
    "\tax2.grid(True)\n",
    "\n",
    "\tfig.suptitle('Residual Analysis', fontsize=18)\n",
    "\tfig.tight_layout()\n",
    "\tfig.savefig(f'../images/residual_analysis_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b7ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plot_residuals(y_val, y_pred, 'xg_reg_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf9334e",
   "metadata": {},
   "source": [
    "**Predicted vs. Actual Plot**\n",
    "\n",
    "There's a dense cluster around 0 for both predicted and actual values, indicating many predictions and centered near 0. However, there is substantial spread both above and below the diagonal line, which suggests underprediction and overprediction. There are clear outliers that are far from the main cluster.\n",
    "\n",
    "\n",
    "**Residual Plot**\n",
    "\n",
    "The residuals show a visible funnel shapes, which indicates a systematic error in prediction. The spread of residuals increases as the predicted delay increases. This is a sign of heteroscedasticity (the variance of errors is not constant across all predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4284c",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40747e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'alpha': [0, 1, 2, 3, 4, 5],\n",
    "    'lambda': [0, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_estimators=100)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    n_iter=25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2682ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "xg_reg_tuned = random_search.best_estimator_\n",
    "xg_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ca1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_tuned.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_tuned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580dd59",
   "metadata": {},
   "source": [
    "The performance didn't improve that much from the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5448a",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a1a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 most important features\n",
    "importances = xg_reg_tuned.get_booster().get_score()\n",
    "importances_df = pd.DataFrame.from_dict(importances, orient='index') \\\n",
    "\t.rename(columns={0: 'importance'}).reset_index() \\\n",
    "\t.rename(columns={'index': 'feature'})\n",
    "importances_df.sort_values('importance', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ad8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances\n",
    "ax = xgb.plot_importance(xg_reg_tuned)\n",
    "ax.figure.tight_layout()\n",
    "ax.figure.savefig('../images/feature_importances_xg_reg_tuned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136884d",
   "metadata": {},
   "source": [
    "**Most Important Features:**\n",
    "- `exp_trip_duration` This is the most important feature in the model. It seems like the expected trip duration is highly predictive of the actual delay. This makes sense as longer expected trips are more prone to disruptions and variations.\n",
    "- `hist_avg_delay` Historical average delay is the second most important predictor. This aligns well with time series predictability since past delays often indicate patterns or bottlenecks that repeat over time.\n",
    "- `route_bearing` The direction of a vehicle might indicate if it's in the direction of traffic or not.\n",
    "- `arrivals_per_hour` The bus frequency is contributing to the prediction. Less frequent buses might be more susceptible to delays since missed connections or unexpected traffic issues tend to accumulate.\n",
    "- `trip_progress` Delays accumulate when the vehicle is further along the trip.\n",
    "\n",
    "\n",
    "**Least Important Features:**\n",
    "- `time_of_day_evening`, `time_of_day_morning`, `time_of_day_night` Evening seems to be a bit more influential than morning or night, which could indicate evening rush hour impacts.\n",
    "- `schedule_relationship_Scheduled` This has low impact, which might indicate that deviations from scheduled times are not systematically captured by the model.\n",
    "\n",
    "- `wheelchair_boarding` Very low importance, indicating it has minimal influence on delays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54f573",
   "metadata": {},
   "source": [
    "## SHAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_plot(shap_values, X_true, model_name:str, barplot:bool=True) -> None:\n",
    "\tif barplot:\n",
    "\t\tshap.summary_plot(shap_values, X_true, plot_type='bar', show=False)\n",
    "\t\tplt.title('SHAP Summary Barplot')\n",
    "\t\tplot_type = 'barplot' \n",
    "\telse: # beeswarm\n",
    "\t\tshap.summary_plot(shap_values, X_true, show=False)\n",
    "\t\tplt.title('SHAP Summary Beeswarm Plot')\n",
    "\t\tplot_type = 'beeswarm_plot' \n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'../images/shap_{plot_type}_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_single_pred(X_true, explainer, shap_values, model_name:str) -> None:\n",
    "\trandom.seed(42)\n",
    "\tindex = random.randrange(len(X_true))\n",
    "\tshap.force_plot(\n",
    "\t\texplainer.expected_value,\n",
    "\t\tshap_values[index, :],\n",
    "\t\tX_true.iloc[index, :],\n",
    "\t\tfigsize=(30, 4),\n",
    "\t\tmatplotlib=True,\n",
    "\t\tshow=False)\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'../images/shap_force_plot_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP\n",
    "X_val_sample = X_val.sample(5000, random_state=42) # sample validation set to prevent memory overload\n",
    "explainer = shap.TreeExplainer(xg_reg_tuned)\n",
    "shap_values = explainer.shap_values(X_val_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary barplot\n",
    "shap_plot(shap_values, X_val_sample, 'xg_reg_tuned', barplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afe1ce",
   "metadata": {},
   "source": [
    "**Comparison with XGBoost feature importances**\n",
    "\n",
    "- Interestingly, `hist_avg_delay` comes out as the most impactful in this plot, while it was second in the XGBoost importance plot.\n",
    "- `arrivals_per_hour` is elevated to the second position, while in the default importance, it is in fourth place.\n",
    "- This suggests that in terms of predictive influence, `hist_avg_delay` and `arrivals_per_hour` are actually more significant than what the XGBoost default metric captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary beeswarm plot\n",
    "shap_plot(shap_values, X_val_sample, 'xg_reg_tuned', barplot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf14bb",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "- High `hist_avg_delay` (red) tends to push predictions higher, and low values (blue) push it lower. The high influence of `hist_avg_delay` confirms that delay is highly dependent on past performance. This could be useful for forecasting in specific segments or optimizing bus routes during peak times.\n",
    "- Some features like `route_bearing` and `exp_avg_delay` have either a positive or negative infuence, suggesting there's a more complex feature interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot a single prediction\n",
    "shap_single_pred(X_val_sample, explainer, shap_values, 'xg_reg_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608349d0",
   "metadata": {},
   "source": [
    "This plot is a breakdown of the specific prediction (`42.36`) for one instance.\n",
    "\n",
    "- Features that increase the prediction (red):\n",
    "\t- `time_of_day_evening`: The trip being in the evening increases the delay.\n",
    "\t- `pressure_msl`: A high atmospheric air pressure (`1010.8`) can have an impact on the vehicle and passenger behaviour.\n",
    "\t- `tempetature_2m`: For this instance, a temperature of `14.2` degrees Celsius increased the delay.\n",
    "\t- `trip_progress`: A trip that's at the end (`0.92`) tend to accumulate delays.\n",
    "\t- `wind_speed_10m`: A wind speed of `9.1` kilometers per hour caused the delay to increase in that case.\n",
    "\n",
    "- Features that decrease the prediction (blue):\n",
    "\t- `exp_trip_duration`: A `3060.0` (51 minute) trip reduced the delay for this instance.\n",
    "\t- `route_bearing`: A value of `112.44` (South-East) decreased the delay.\n",
    "\t- `arrivals_per_hour`: A bus that arrives less frequently (`2.0` times per hour) is less susceptible to delays.\n",
    "\t- `hist_avg_delay`: A value of `47.64` decreased the delay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac48272",
   "metadata": {},
   "source": [
    "## Feature Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c13bde",
   "metadata": {},
   "source": [
    "### Add feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c8fa3b",
   "metadata": {},
   "source": [
    "Some features were surprisingly low impact and the SHAP plots suggest there might be interactions between some features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate second degree polynomial features\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_train_poly = poly.transform(X_train)\n",
    "X_val_poly = poly.transform(X_val)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "\n",
    "feature_names = poly.get_feature_names_out(X.columns)\n",
    "X_poly = pd.DataFrame(X_poly, columns=feature_names)\n",
    "X_train_poly = pd.DataFrame(X_train_poly, columns=feature_names)\n",
    "X_val_poly = pd.DataFrame(X_val_poly, columns=feature_names)\n",
    "X_test_poly = pd.DataFrame(X_test_poly, columns=feature_names)\n",
    "\n",
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit model\n",
    "xg_reg_poly = xgb.XGBRegressor(\n",
    "  \tobjective='reg:squarederror',\n",
    "  \trandom_state=42,\n",
    "  \tn_estimators=100,\n",
    "    max_depth=xg_best_params['max_depth'],\n",
    "    learning_rate=xg_best_params['learning_rate'],\n",
    "    subsample=xg_best_params['subsample'],\n",
    "    colsample_bytree=xg_best_params['colsample_bytree'],\n",
    "    alpha=xg_best_params['alpha'],\n",
    "    reg_lambda=xg_best_params['lambda']\n",
    ")\n",
    "\n",
    "xg_reg_poly.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbfcad4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xg_reg_base</td>\n",
       "      <td>69.204192</td>\n",
       "      <td>135.831290</td>\n",
       "      <td>0.282591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xg_reg_poly</td>\n",
       "      <td>66.992521</td>\n",
       "      <td>132.338056</td>\n",
       "      <td>0.319016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model        MAE        RMSE        R²\n",
       "0  xg_reg_base  69.204192  135.831290  0.282591\n",
       "1  xg_reg_poly  66.992521  132.338056  0.319016"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_poly.predict(X_val_poly)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_poly')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c23ac5",
   "metadata": {},
   "source": [
    "There is a slight improvement from the model without the interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0adcb9",
   "metadata": {},
   "source": [
    "### Keep best interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba73384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Mutual Information to rank feature importance\n",
    "mi_scores = mutual_info_regression(X_val_poly, y_val) # use validation set to save computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e07c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "mi_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'mi_score': mi_scores\n",
    "}).sort_values(by='mi_score', ascending=False)\n",
    "mi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75dc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 10 interactions\n",
    "top_10_features = mi_df.head(10)['feature'].tolist()\n",
    "X_top_10 = X_poly[top_10_features]\n",
    "X_top_10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d22350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a heatmap of the correlations between these top features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X_top_10.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap of Top 10 Interaction Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb497bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features with a score greater than 0.1\n",
    "best_features = mi_df[mi_df['mi_score'] > 0.1]['feature'].sort_values().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep best features\n",
    "X_best = X_poly[best_features]\n",
    "X_train_best = X_train_poly[best_features]\n",
    "X_val_best = X_val_poly[best_features]\n",
    "X_test_best = X_test_poly[best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24afa32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most correlated interactions\n",
    "correlations = get_top_abs_correlations(X_best)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bdb657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features\n",
    "features_to_drop = list(set(correlations.index.get_level_values(level=1)))\n",
    "\n",
    "X_best = X_best.drop(features_to_drop, axis=1)\n",
    "X_train_best = X_train_best.drop(features_to_drop, axis=1)\n",
    "X_val_best = X_val_best.drop(features_to_drop, axis=1)\n",
    "X_test_best = X_test_best.drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec1d96",
   "metadata": {},
   "source": [
    "## Retrain Model with Best Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254cf819",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit model\n",
    "xg_reg_pruned = xgb.XGBRegressor(\n",
    "  \tobjective='reg:squarederror',\n",
    "  \trandom_state=42,\n",
    "  \tn_estimators=100,\n",
    "    max_depth=xg_best_params['max_depth'],\n",
    "    learning_rate=xg_best_params['learning_rate'],\n",
    "    subsample=xg_best_params['subsample'],\n",
    "    colsample_bytree=xg_best_params['colsample_bytree'],\n",
    "    alpha=xg_best_params['alpha'],\n",
    "    reg_lambda=xg_best_params['lambda']\n",
    ")\n",
    "\n",
    "xg_reg_pruned.fit(X_train_best, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682dcd34",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xg_reg_pruned.predict(X_val_best)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_pruned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1dea0",
   "metadata": {},
   "source": [
    "The RMSE and R-squared improved after removing interactions, which is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff227ef8",
   "metadata": {},
   "source": [
    "### Hyper-tune model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'alpha': [0, 1, 2, 3, 4, 5],\n",
    "    'lambda': [0, 1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, n_estimators=100)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    n_iter=25,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_best, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879fda38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "xg_reg_pruned_tuned = random_search.best_estimator_\n",
    "xg_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f5d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_pruned_tuned.predict(X_val_best)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_pruned_tuned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2896f",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 most important features\n",
    "importances = xg_reg_pruned_tuned.get_booster().get_score()\n",
    "importances_df = pd.DataFrame.from_dict(importances, orient='index') \\\n",
    "\t.rename(columns={0: 'importance'}).reset_index() \\\n",
    "\t.rename(columns={'index': 'feature'})\n",
    "importances_df.sort_values('importance', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664a3b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances\n",
    "ax = xgb.plot_importance(xg_reg_pruned_tuned, max_num_features=20)\n",
    "ax.figure.tight_layout()\n",
    "ax.figure.savefig('../images/feature_importances_xg_reg_pruned_tuned.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7a2d2",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "\n",
    "- `arrivals_per_hour exp_trip_duration` is by far the most important. This suggests that the number of arrivals combined with the expected trip duration captures strong predictive patterns.\n",
    "- The interaction `arrivals_per_hour hist_avg_delay` is second, indicating that congestion or frequent arrivals play a big role when paired with historical average delays.\n",
    "- Weather (`temperature_2m`) is also heavily impactful when combined with `trip_duration` and `route_bearing`, showing that environmental conditions are critical when predicting delays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041da79",
   "metadata": {},
   "source": [
    "### SHAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be8e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP\n",
    "sample_size = 5000 # sample test set to prevent memory overload\n",
    "X_val_sample = X_val_best.sample(n=sample_size, random_state=42) \n",
    "explainer = shap.TreeExplainer(xg_reg_pruned)\n",
    "shap_values = explainer.shap_values(X_val_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1fa82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary barplot\n",
    "shap_plot(shap_values, X_val_sample, 'xg_reg_pruned_tuned', barplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb4ce5b",
   "metadata": {},
   "source": [
    "- Compared to XGBoost default importance, the top interaction shifts: `cloud_cover hist_avg_delay` is the most impactful in SHAP, even though it was not the highest in XGBoost's internal ranking.\n",
    "- `route_bearing stop_cluster` and `route_bearing temperature_2m` also surface as strong influences.\n",
    "- XGBoost splits more on `arrivals_per_hour exp_trip_duration`, but SHAP reveals that `cloud cover` and `historical delays` are actually more predictive.\n",
    "This suggests that XGBoost may prioritize frequent splits on interactions that help its tree-building, but SHAP captures the real impact on the prediction values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary beeswarm plot\n",
    "shap_plot(shap_values, X_val_sample, 'xg_reg_pruned_tuned', barplot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f579ba3b",
   "metadata": {},
   "source": [
    "- For `cloud_cover hist_avg_delay`: High values of both features (red) seem to push the prediction higher, which is visible with strong skew to the right.\n",
    "- For `route_bearing stop_cluster`: Certain clusters, when combined with specific route bearings, consistently push predictions higher or lower.\n",
    "- `route_bearing temperature_2m`: High temperatures combined with specific route bearings have a strong influence.\n",
    "- `exp_trip_duration route_bearing`: Longer trip durations on specific bearings heavily impact the prediction, possibly capturing peak-hour traffic or route-specific congestion.\n",
    "- `arrivals_per_hour hist_avg_delay`: This aligns with intuition—more buses arriving in historically delayed spots leads to higher expected delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot a single prediction\n",
    "shap_single_pred(X_val_sample, explainer, shap_values, 'xg_reg_pruned_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a0186",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb04b33",
   "metadata": {},
   "source": [
    "### Retrain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "xg_train_data = xgb.DMatrix(X_train_best, y_train, enable_categorical=False)\n",
    "xg_val_data = xgb.DMatrix(X_val_best, y_val, enable_categorical=False)\n",
    "xg_test_data = xgb.DMatrix(X_test_best, y_test, enable_categorical=False)\n",
    "xg_eval_set = [(xg_train_data, 'train'), (xg_val_data, 'validation')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c906fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with more boosting rounds\n",
    "final_model = xgb.train(\n",
    "  params= {\n",
    "    'objective':'reg:squarederror', \n",
    "  \t'tree_method': 'hist',\n",
    "    'max_depth': xg_best_params['max_depth'],\n",
    "    'learning_rate': xg_best_params['learning_rate'],\n",
    "    'subsample': xg_best_params['subsample'],\n",
    "    'colsample_bytree': xg_best_params['colsample_bytree'],\n",
    "\t'alpha': xg_best_params['alpha'],\n",
    "    'lambda': xg_best_params['lambda'],\n",
    "  },\n",
    "  dtrain=xg_train_data,\n",
    "  num_boost_round=10000,\n",
    "  evals=xg_eval_set,\n",
    "  verbose_eval=50,\n",
    "  early_stopping_rounds=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ac4ed",
   "metadata": {},
   "source": [
    "### Evaluate with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a96346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = final_model.predict(xg_test_data)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_test, 'xg_reg_final')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353241a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plot_residuals(y_test, y_pred, 'xg_reg_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e5c92",
   "metadata": {},
   "source": [
    "### Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display features\n",
    "best_features = X_best.columns.tolist()\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "test_input = {\n",
    "#   \t'cloud_cover': [0],\n",
    "# \t'exp_trip_duration': [3600],\n",
    "#   \t'frequency_normal': [1],\n",
    "\t\n",
    "\t\n",
    "# \t'relative_humidity_2m': [60],\n",
    "# \t'wind_direction_10m': [140],\n",
    "# \t'precipitation': [0],\n",
    "# \t'time_of_day_morning': [0],\n",
    "# \t'hist_avg_delay': [300],\n",
    "# \t'route_direction_South': [0],\n",
    "# \t'wind_speed_10m': [10],\n",
    "\t\n",
    "# \t'time_of_day_evening': [0],\n",
    "# \t'stop_location_group': [2],\n",
    "# \t'is_peak_hour': [1],\n",
    "# \t'trip_phase_middle': [0],\n",
    "# \t'frequency_very_rare': [0],\n",
    "# \t'route_direction_North': [0],\n",
    "# \t'route_direction_West': [1],\n",
    "# \t'frequency_rare': [0],\n",
    "# \t'temperature_2m': [24.3],\n",
    "# \t'stop_distance': [400],\n",
    "\t\n",
    "# \t'trip_phase_start': [0]\n",
    "}\n",
    "\n",
    "x_test = pd.DataFrame(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb883fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict delay\n",
    "prediction = final_model.predict(x_test)\n",
    "print(f'Predicted delay: {prediction[0]:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8a024",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, hyperparameters and predictors\n",
    "joblib.dump(final_model, '../models/regression_model.pkl')\n",
    "joblib.dump(xg_best_params, '../models/best_hyperparams.pkl')\n",
    "joblib.dump(best_features, '../models/best_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e39b53",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
