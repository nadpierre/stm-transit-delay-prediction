{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6efc2fa1",
   "metadata": {},
   "source": [
    "# STM Transit Delay Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48c78c",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4739c1",
   "metadata": {},
   "source": [
    "This notebook explores tree-based machine learning models in order to find the one that predicts STM transit delays with the best accuracy. The featured models are XGBoost, LightGBM and CatBoost, because they are more suitable for large datasets with mixed data and high cardinality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec250c6",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f649494",
   "metadata": {},
   "source": [
    "`exp_trip_duration`: Expected duration of a trip, in seconds.<br>\n",
    "`route_direction_North`, `route_direction_South`, `route_direction_West`: Route direction in degrees.<br>\n",
    "`route_type_Night`, `route_type_HighFrequency` : One-Hot features for types of bus lines<br>\n",
    "`frequency_frequent`, `frequency_normal`, `frequency_rare`, `frequency_very_frequent`, `frequency_very_rare`: One-Hot features for number of arrivals per hour.<br>\n",
    "`stop_location_group`: Stop cluster based on coordinates.<br>\n",
    "`stop_distance`: Distance between the previous and current stop, in meters.<br>\n",
    "`trip_phase_middle`, `trip_phase_end`: One-Hot feature for trip progress.<br>\n",
    "`exp_delay_prev_stop`: Expected duration between the previous and current stop, in seconds.<br>\n",
    "`wheelchair_boarding`: Indicates if the stop is accessible for people in wheelchair.<br>\n",
    "`sch_rel_Scheduled`: One-Hot feature for schedule relationship.<br>\n",
    "`time_of_day_evening`, `time_of_day_morning`, `time_of_day_night`: One-Hot features for time of day.<br>\n",
    "`is_peak_hour`: Boolean value indicating if the sheduled arrival time is at peak hour.<br>\n",
    "`temperature_2m`: Air temperature at 2 meters above ground, in Celsius.<br>\n",
    "`relative_humidity_2m`: Relative humidity at 2 meters above ground, in percentage.<br>\n",
    "`precipitation`: Total precipitation (rain, showers, snow) sum of the preceding hour, in millimeters.<br>\n",
    "`pressure_msl`: Atmospheric air pressure reduced to mean sea level (msl), in hPa.<br>\n",
    "`cloud_cover`: Total cloud cover as an area fraction.<br>\n",
    "`windspeed_10m`: Wind speed at 10 meters above ground, in kilometers per hour.<br>\n",
    "`wind_direction_10m`: Wind direction at 10 meters above ground.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d218f9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dded95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shap\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d19b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('../data/preprocessed.parquet')\n",
    "print(f'Shape of dataset: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc11b74",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51490206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target variable\n",
    "X = df.drop('delay', axis=1)\n",
    "y = df['delay']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4a098",
   "metadata": {},
   "source": [
    "The 3 models can run multiple iterations with a training and validation set. Therefore, a hold-out set will be kept to evaluate the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee2060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split (60-20-20)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "del X_temp\n",
    "del y_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817f9998",
   "metadata": {},
   "source": [
    "**Scaling**\n",
    "\n",
    "Since only tree-based models are explored in this project, scaling is not needed because the models are not sensitive to the absolute scale or distribution of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a69b3",
   "metadata": {},
   "source": [
    "## Fit Base Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce7b6b",
   "metadata": {},
   "source": [
    "All models allow to setup a number of rounds and early stopping. To start, all models will run 100 rounds with an early stopping of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to track metrics\n",
    "metrics_df = pd.DataFrame(columns=['model', 'MAE', 'RMSE', 'R²'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_reg_metrics(metrics_df:pd.DataFrame, y_pred:pd.Series, y_true:pd.Series, model_name:str) -> pd.DataFrame:\n",
    "\tmae = mean_absolute_error(y_true, y_pred)\n",
    "\trmse = root_mean_squared_error(y_true, y_pred)\n",
    "\tr2 = r2_score(y_true, y_pred)\n",
    "\n",
    "\tmetrics_df.loc[len(metrics_df)] = [model_name, mae, rmse, r2]\n",
    "\treturn metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f3056",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d4f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression matrices\n",
    "xg_train_data = xgb.DMatrix(X_train, y_train, enable_categorical=False)\n",
    "xg_val_data = xgb.DMatrix(X_val, y_val, enable_categorical=False)\n",
    "xg_test_data = xgb.DMatrix(X_test, y_test, enable_categorical=False)\n",
    "xg_eval_set = [(xg_train_data, 'train'), (xg_val_data, 'validation')]\n",
    "xg_test_set = [(xg_train_data, 'train'), (xg_test_data, 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "xg_reg_base = xgb.train(\n",
    "  params= {'objective': 'reg:squarederror', 'tree_method': 'hist'},\n",
    "  dtrain=xg_train_data,\n",
    "  num_boost_round=100,\n",
    "  evals=xg_eval_set,\n",
    "  verbose_eval=10,\n",
    "  early_stopping_rounds=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_base.predict(xg_val_data)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0a5e7",
   "metadata": {},
   "source": [
    "**MAE**<br>\n",
    "On average, the predictions are off by 70 seconds, which is reasonable, knowing that [STM](https://www.stm.info/en/info/networks/bus-network-and-schedules-enlightened) considers a bus arriving 3 minutes after the planned schedule as being on time.\n",
    "\n",
    "**RMSE**<br>\n",
    "The higher RMSE compared to MAE suggests that there are some significant prediction errors that influence the overall error metric.\n",
    "\n",
    "**R²**<br>\n",
    "The model explains 24.35% of the variance, which is not good but understandable because of how random transit delays can be (bad weather, vehicle breakdown, accidents, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960dd759",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3edbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regression datasets\n",
    "lgb_train_data = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_val_data = lgb.Dataset(X_val, label=y_val, reference=lgb_train_data)\n",
    "lgb_test_data = lgb.Dataset(X_test, label=y_test, reference=lgb_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32750cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "lgb_reg_base = lgb.train(\n",
    "    params={\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': -1\n",
    "    },\n",
    "    train_set=lgb_train_data,\n",
    "    valid_sets=[lgb_val_data],\n",
    "    num_boost_round=100,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a86d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = lgb_reg_base.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'lgb_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f15c1",
   "metadata": {},
   "source": [
    "The LightGBM model performs worse than XGBoost, especially in terms of R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a61f6b",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b34ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "cat_reg_base = CatBoostRegressor(\n",
    "    iterations=100,\n",
    "    learning_rate=0.05,\n",
    "    depth=10,\n",
    "    random_seed=42,\n",
    "    verbose=10\n",
    ")\n",
    "\n",
    "cat_reg_base.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = cat_reg_base.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'cat_reg_base')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e751ec",
   "metadata": {},
   "source": [
    "CatBoost performs almost like LightGBM. So far, XGBoost seems to capture a bit more of the underlying patterns than the two other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af4284c",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd661f2f",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40747e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=2,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2682ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "xg_reg_tuned = random_search.best_estimator_\n",
    "xg_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ca1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = xg_reg_tuned.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'xg_reg_tuned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b84a0",
   "metadata": {},
   "source": [
    "There's a significant improvement from the base XGBoost model and it's the best performing model so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688cf44e",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "  'n_estimators': [100, 200, 300, 400, 500, 600],\n",
    "  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  'max_depth': [5, 10, 15],\n",
    "  'num_leaves': [20, 31, 40],\n",
    "  'min_child_samples': [10, 20, 30],\n",
    "  'subsample': [0.8, 1.0],\n",
    "  'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=2, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c8f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "lgb_reg_tuned = random_search.best_estimator_\n",
    "lgb_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = lgb_reg_tuned.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'lgb_reg_tuned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaf4eee",
   "metadata": {},
   "source": [
    "The performance is very similar to the previous tuned model. The MAE is slightly worse but the RMSE and the R-squared are slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8d5086",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d6dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "  'num_trees': [100, 200, 300, 400, 500, 600],\n",
    "  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  'depth': [6, 8, 10],\n",
    "  'l2_leaf_reg': [1, 3, 5],\n",
    "  'border_count': [32, 64, 128],\n",
    "  'bagging_temperature': [0, 1, 5],\n",
    "}\n",
    "\n",
    "cat_model = CatBoostRegressor(verbose=0, random_seed=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=cat_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=2,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcb762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "cat_reg_tuned = random_search.best_estimator_\n",
    "cat_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = cat_reg_tuned.predict(X_val)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'cat_reg_tuned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a90fcf",
   "metadata": {},
   "source": [
    "After tuning, LightGBM is the best model. This is the model that will be used for the rest of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebbac9",
   "metadata": {},
   "source": [
    "## Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0854867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "best_model = lgb_reg_tuned\n",
    "y_pred = best_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180dd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(y_true, y_pred, model_name:str) -> None:\n",
    "\tfig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n",
    "\n",
    "\t# Predicted vs. actual values\n",
    "\tax1.scatter(x=y_pred, y=y_true)\n",
    "\tax1.set_title('Predicted vs. Actual values')\n",
    "\tax1.set_xlabel('Predicted Delay (seconds)')\n",
    "\tax1.set_ylabel('Actual Delay (seconds)')\n",
    "\tax1.grid(True)\n",
    "\n",
    "\t# Residuals\n",
    "\tresiduals = y_true - y_pred\n",
    "\tax2.scatter(x=y_pred, y=residuals)\n",
    "\tax2.set_title('Residual Plot')\n",
    "\tax2.set_xlabel('Predicted Delay (seconds)')\n",
    "\tax2.set_ylabel('Residuals (seconds)')\n",
    "\tax2.axhline(0, linestyle='--', color='orange')\n",
    "\tax2.grid(True)\n",
    "\n",
    "\tfig.suptitle('Residual Analysis', fontsize=18)\n",
    "\tfig.tight_layout()\n",
    "\tfig.savefig(f'../images/residual_analysis_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plot_residuals(y_val, y_pred, 'cat_reg_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52270f20",
   "metadata": {},
   "source": [
    "**Predicted vs. Actual Plot**\n",
    "\n",
    "There's a dense cluster around 0 for both predicted and actual values, indicating many predictions and centered near 0. However, there is substantial spread both above and below the diagonal line, which suggests underprediction and overprediction. There are clear outliers that are far from the main cluster.\n",
    "\n",
    "\n",
    "**Residual Plot**\n",
    "\n",
    "The residuals show a visible funnel shapes, which indicates a systematic error in prediction. The spread of residuals increases as the predicted delay increases. This is a sign of heteroscedasticity (the variance of errors is not constant across all predictions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5448a",
   "metadata": {},
   "source": [
    "## Feature Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf3fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feat_importance(feature_importances, model_name:str) -> None:\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.barh(feature_importances['Feature Id'], feature_importances['Importances'])\n",
    "\tplt.gca().invert_yaxis()\n",
    "\tplt.title('Feature Importance')\n",
    "\tplt.xlabel('Importance')\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'../images/feature_importances_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sorted feature importances\n",
    "feature_importances = best_model.get_feature_importance(prettified=True)\n",
    "feature_importances = feature_importances.sort_values(by='Importances', ascending=False)\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ad8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importance\n",
    "plot_feat_importance(feature_importances, 'cat_reg_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4136884d",
   "metadata": {},
   "source": [
    "**Most Important Features:**\n",
    "- `exp_trip_duration` This is the most important feature in the model. It seems like the expected trip duration is highly predictive of the actual delay. This makes sense as longer expected trips are more prone to disruptions and variations.\n",
    "- `hist_avg_delay` Historical average delay is the second most important predictor. This aligns well with time series predictability since past delays often indicate patterns or bottlenecks that repeat over time.\n",
    "- `stop_location_group` This is also highly influential. Grouping the stops by location might be capturing specific problematic areas or geographic patterns that contribute to delays (e.g., heavy traffic zones, construction areas, major intersections).\n",
    "- `temperature_2m`, `wind_direction_10m` Weather conditions do play a role, but not as heavily as trip-related features. The fact that wind direction and temperature are relatively impactful suggests weather variability might affect delays more than just precipitation alone.\n",
    "\n",
    "**Least Important Features:**\n",
    "- `is_peak_hour` This is surprisingly less impactful than expected. It suggests that perhaps peak hours are not as unpredictable as other features.\n",
    "- `time_of_day_morning`, `time_of_day_night` Evening seems to be more influential than morning or night, which could indicate evening rush hour impacts.\n",
    "- `frequency_very_frequent` The bus frequency is contributing to the prediction. More frequent buses might be less susceptible to delays since missed connections or unexpected traffic issues don't accumulate as much.\n",
    "- `wheelchair_boarding` Very low importance, indicating it has minimal influence on delays.\n",
    "- `schedule_relationship_Scheduled` This has almost no impact, which might indicate that deviations from scheduled times are not systematically captured by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da54f573",
   "metadata": {},
   "source": [
    "## SHAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc0ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_plot(shap_values, X_true, model_name:str, barplot:bool=True) -> None:\n",
    "\tif barplot:\n",
    "\t\tshap.summary_plot(shap_values, X_true, plot_type='bar', show=False)\n",
    "\t\tplt.title('SHAP Summary Barplot')\n",
    "\t\tplot_type = 'barplot' \n",
    "\telse: # beeswarm\n",
    "\t\tshap.summary_plot(shap_values, X_true, show=False)\n",
    "\t\tplt.title('SHAP Summary Beeswarm Plot')\n",
    "\t\tplot_type = 'beeswarm_plot' \n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'../images/shap_{plot_type}_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173a738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_single_pred(X_true, explainer, shap_values, model_name:str) -> None:\n",
    "\tindex = random.randrange(len(X_true))\n",
    "\tshap.force_plot(\n",
    "\t\texplainer.expected_value,\n",
    "\t\tshap_values[index, :],\n",
    "\t\tX_true.iloc[index, :],\n",
    "\t\tfigsize=(15, 4),\n",
    "\t\tcontribution_threshold=0.075,\n",
    "\t\tmatplotlib=True,\n",
    "\t\tshow=False)\n",
    "\tplt.tight_layout()\n",
    "\tplt.savefig(f'../images/shap_force_plot_{model_name}.png', bbox_inches='tight')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9e7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP\n",
    "sample_size = 250 # sample validation set to prevent memory overload\n",
    "X_val_sample = X_val.sample(n=sample_size, random_state=42) \n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_val_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810802f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary barplot\n",
    "shap_plot(shap_values, X_val_sample, 'cat_reg_tuned', barplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60afe1ce",
   "metadata": {},
   "source": [
    "- The ranking matches the feature importances plot:\n",
    "\t- `hist_avg_delay` and `exp_trip_duration` dominate the list, with a substantial gap from the rest.\n",
    "\t- `frequency_very_rare` and `stop_location_group` are also critical.\n",
    "\t- Weather (`temperature_2m`, `wind_speed_10m`) plays a moderate role.\n",
    "\n",
    "**Insight:**\n",
    "\n",
    "The impact of weather is not that high. It's probably because the data has been collected in spring and usually, there are not extreme weather conditions at that time of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary beeswarm plot\n",
    "shap_plot(shap_values, X_val_sample, 'cat_reg_tuned', barplot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bf14bb",
   "metadata": {},
   "source": [
    "The beeswarm plot offers a global view of all predictions and their feature influences.\n",
    "\n",
    "- `hist_avg_delay` and `exp_trip_duration` are the top features:\n",
    "\t- High values of hist_avg_delay (in red) push predictions higher.\n",
    "\t- For `exp_trip_duration`, high values both increase and decrease the delay prediction, indicating complex interactions.\n",
    "- Color Representation:\n",
    "\t- Red = High feature value, Blue = Low feature value.\n",
    "\t- For example, when `route_direction_West` is true, it pushes the prediction up. When it is low, it has little to no effect.\n",
    "- Weather Variables:\n",
    "\t- `temperature_2m`, `wind_speed_10m`, and `wind_direction_10m` also affect predictions. For instance, higher wind speeds push predictions slightly upwards, which makes sense given that weather disturbances can slow down traffic.\n",
    "- Time of Day:\n",
    "\t- Evening seems to affect delay more than morning or night, aligning with typical rush hour traffic.\n",
    "\n",
    "**Insight:**\n",
    "\n",
    "The high influence of `hist_avg_delay` confirms that delay is highly dependent on past performance. This could be useful for forecasting in specific segments or optimizing bus routes during peak times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot a single prediction\n",
    "shap_single_pred(X_val_sample, explainer, shap_values, 'cat_reg_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608349d0",
   "metadata": {},
   "source": [
    "This plot is a breakdown of the specific prediction (`68.23`) for one instance.\n",
    "\n",
    "- Features that increase the prediction (red):\n",
    "\t- `route_direction_West`: When the bus is going West, it strongly increases the delay.\n",
    "\t- `frequency_very_rare`: If the bus service is rare, it also increases the expected delay.\n",
    "\t- `hist_avg_delay`: A large historical average delay of `100.59` also pushes the prediction up significantly.\n",
    "- Features that decrease the prediction (blue):\n",
    "\t- `stop_location_group`: A value of `1.0` (extreme West of Montreal) for the stop group reduces the delay.\n",
    "\t- `exp_trip_duration`: An expected trip duration of `3960.0` seconds also reduces the delay, which is a bit surprising. This might indicate that long trips in this grouping tend to be managed better.<br><br>\n",
    "\n",
    "**Insight:**\n",
    "\n",
    "The model predicts more delay (`68.23`) when:\n",
    "- The bus heads West.\n",
    "- It is part of a rare frequency group.\n",
    "- Historical delays have been high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac48272",
   "metadata": {},
   "source": [
    "## Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717743e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive feature elimination with a patience of 2\n",
    "\n",
    "X_current = X.copy()\n",
    "X_train_current = X_train.copy()\n",
    "X_val_current = X_val.copy()\n",
    "X_test_current = X_test.copy()\n",
    "\n",
    "best_rmse = float(metrics_df['RMSE'].min())\n",
    "best_features = X_current.columns.tolist()\n",
    "tracking = [] # To store number of features and RMSE\n",
    "patience = 2\n",
    "patience_counter = 0\n",
    "\n",
    "keep_going = True\n",
    "\n",
    "while keep_going and len(X_current.columns) > 10: # Keep at least 10 features\n",
    "\tcat_model = CatBoostRegressor(\n",
    "\t\titerations=cat_best_params['iterations'],\n",
    "\t\tlearning_rate=cat_best_params['learning_rate'],\n",
    "\t\tdepth=cat_best_params['depth'],\n",
    "\t\tl2_leaf_reg=cat_best_params['l2_leaf_reg'],\n",
    "\t\tborder_count=cat_best_params['border_count'],\n",
    "\t\tbagging_temperature=cat_best_params['bagging_temperature'],\n",
    "\t\trandom_seed=42,\n",
    "\t\tverbose=10\n",
    "\t)\n",
    "\n",
    "\tcat_model.fit(X_train_current, y_train, eval_set=(X_val_current, y_val), early_stopping_rounds=3)\n",
    "\n",
    "\t# Predict and calculate RMSE\n",
    "\ty_pred = cat_model.predict(X_val_current)\n",
    "\trmse = root_mean_squared_error(y_val, y_pred)\n",
    "\ttracking.append({\n",
    "\t\t'nb_features': len(X_current.columns),\n",
    "\t\t'RMSE': best_rmse\n",
    "\t})\n",
    "\n",
    "\t# Feature importance\n",
    "\timportances = cat_model.get_feature_importance()\n",
    "\tweakest_feature = importances.idxmin()\n",
    "\tweakest_score = importances.min()\n",
    "\n",
    "\tif rmse <= best_rmse:\n",
    "\t\tbest_rmse = rmse\n",
    "\t\tbest_features = X_current.columns.tolist()\n",
    "\n",
    "\t\t# Drop the weakest feature\n",
    "\t\tprint(f'RMSE: {rmse:.4f} | Dropping: {weakest_feature} (importance {importances.min():.6f})')\n",
    "\t\tX_current = X_current.drop(columns=[weakest_feature])\n",
    "\t\tX_train_current = X_train_current.drop(columns=[weakest_feature])\n",
    "\t\tX_val_current = X_val_current.drop(columns=[weakest_feature])\n",
    "\t\tX_test_current = X_test_current.drop(columns=[weakest_feature])\n",
    "\n",
    "\t\tpatience_counter = 0 # Reset patience if RMSE improves\n",
    "\t\n",
    "\telse:\n",
    "\t\tpatience_counter += 1\n",
    "\t\tprint(f'Patience counter: {patience_counter}/{patience}')\n",
    "\t\n",
    "\t\tif patience_counter >= patience:\n",
    "\t\t\tprint('Performance worsened. Stopping feature elimination.')\n",
    "\t\t\tkeep_going = False\n",
    "\t\telse:\n",
    "\t\t\t# Allow two bad steps: still drop feature and continue\n",
    "\t\t\tX_current = X_current.drop(columns=[weakest_feature])\n",
    "\t\t\tX_train_current = X_train_current.drop(columns=[weakest_feature])\n",
    "\t\t\tX_val_current = X_val_current.drop(columns=[weakest_feature])\n",
    "\t\t\tX_test_current = X_test_current.drop(columns=[weakest_feature])\n",
    "\n",
    "\n",
    "print('\\nBest set of features found:\\n')\n",
    "print(best_features)\n",
    "print(f'Final validation RMSE: {best_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d25fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE vs. number of features\n",
    "tracking_df = pd.DataFrame(tracking)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(tracking_df['nb_features'], tracking_df['RMSE'], marker='o')\n",
    "plt.gca().invert_xaxis()  # More features on the left\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Validation RMSE')\n",
    "plt.title('Recursive Feature Elimination Progress')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'../images/rfe_rmse_tracking.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "66a90f46",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Identify low impact features (below 0.005)\n",
    "low_impact_features = feature_importances[feature_importances['Importances'] < 1]\n",
    "\n",
    "print('One-hot features with low SHAP impact to remove:\\n')\n",
    "print(low_impact_features)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28bf5e73",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Keep best features\n",
    "features_to_drop = low_impact_features['feature'].tolist()\n",
    "all_features = X.columns.tolist()\n",
    "\n",
    "diff = set(all_features) - set(features_to_drop)\n",
    "best_features = list(diff)\n",
    "best_features = sorted(best_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9041f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns from input matrices\n",
    "X_pruned = X[best_features]\n",
    "X_train_pruned = X_train[best_features]\n",
    "X_val_pruned = X_val[best_features]\n",
    "X_test_pruned = X_test[best_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcec1d96",
   "metadata": {},
   "source": [
    "## Retrain Model with Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b229b6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain Model\n",
    "cat_reg_pruned = CatBoostRegressor(\n",
    "    iterations=10000,\n",
    "    learning_rate=cat_best_params['learning_rate'],\n",
    "    depth=cat_best_params['depth'],\n",
    "    l2_leaf_reg=cat_best_params['l2_leaf_reg'],\n",
    "    border_count=cat_best_params['border_count'],\n",
    "    bagging_temperature=cat_best_params['bagging_temperature'],\n",
    "    random_seed=42,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "cat_reg_tuned.fit(X_train_pruned, y_train, eval_set=(X_val_pruned, y_val), early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bc463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = cat_reg_tuned.predict(X_val_pruned)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_val, 'cat_reg_pruned')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463cacbc",
   "metadata": {},
   "source": [
    "The model metrics with less features are similar, which means eliminating features didn't worsen the performance too much."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f2a87",
   "metadata": {},
   "source": [
    "## Retune Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "  'iterations': [50, 100],\n",
    "  'learning_rate': [0.01, 0.05, 0.1],\n",
    "  'depth': [6, 8, 10],\n",
    "  'l2_leaf_reg': [1, 3, 5],\n",
    "  'border_count': [32, 64, 128],\n",
    "  'bagging_temperature': [0, 1, 5],\n",
    "}\n",
    "\n",
    "cat_model = CatBoostRegressor(verbose=0, random_seed=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=cat_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,\n",
    "    cv=2,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train_pruned, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df11718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "cat_pruned_best_model = random_search.best_estimator_\n",
    "cat_pruned_best_params = random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebd7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain final model with more iterations\n",
    "cat_reg_final = CatBoostRegressor(\n",
    "    iterations=10000,\n",
    "    learning_rate=cat_pruned_best_params['learning_rate'],\n",
    "    depth=cat_pruned_best_params['depth'],\n",
    "    l2_leaf_reg=cat_pruned_best_params['l2_leaf_reg'],\n",
    "    border_count=cat_pruned_best_params['border_count'],\n",
    "    bagging_temperature=cat_pruned_best_params['bagging_temperature'],\n",
    "    random_seed=42,\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "cat_reg_final.fit(X_train_pruned, y_train, eval_set=(X_val_pruned, y_val), early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a0186",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ac4ed",
   "metadata": {},
   "source": [
    "### Evaluate with Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b849001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = cat_reg_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a96346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = final_model.predict(X_test_pruned)\n",
    "\n",
    "metrics_df = add_reg_metrics(metrics_df, y_pred, y_test, 'cat_reg_final')\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353241a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot residuals\n",
    "plot_residuals(y_test, y_pred, 'cat_reg_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1902a7",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590f5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 most important features\n",
    "feature_importances = final_model.get_feature_importance(prettified=True)\n",
    "feature_importances = feature_importances.sort_values(by='Importances', ascending=False)\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b451390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importance\n",
    "plot_feat_importance(feature_importances, 'cat_reg_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d0820b",
   "metadata": {},
   "source": [
    "### SHAP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7fbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP\n",
    "sample_size = 250 # sample validation set to prevent memory overload\n",
    "X_test_sample = X_test_pruned.sample(n=sample_size, random_state=42) \n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e0608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary barplot\n",
    "shap_plot(shap_values, X_test_sample, 'cat_reg_final', barplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325648ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary beeswarm plot\n",
    "shap_plot(shap_values, X_test_sample, 'cat_reg_final', barplot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force plot a single prediction\n",
    "shap_single_pred(X_test_sample, explainer, shap_values, 'cat_reg_final')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9e5c92",
   "metadata": {},
   "source": [
    "### Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eff1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stop coordinates scaler\n",
    "scaler_coords = joblib.load('../models/scaler_coords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65010d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display features\n",
    "best_features = X_test_pruned.columns.tolist()\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8556bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "test_input = {\n",
    "\t'exp_trip_duration': [3600],\n",
    "\t'relative_humidity_2m': [60],\n",
    "\t'wind_direction_10m': [140],\n",
    "\t'precipitation': [0],\n",
    "\t'time_of_day_morning': [0],\n",
    "\t'hist_avg_delay': [300],\n",
    "\t'route_direction_South': [0],\n",
    "\t'wind_speed_10m': [10],\n",
    "\t'frequency_normal': [1],\n",
    "\t'time_of_day_evening': [0],\n",
    "\t'stop_location_group': [2],\n",
    "\t'is_peak_hour': [1],\n",
    "\t'trip_phase_middle': [0],\n",
    "\t'frequency_very_rare': [0],\n",
    "\t'route_direction_North': [0],\n",
    "\t'route_direction_West': [1],\n",
    "\t'frequency_rare': [0],\n",
    "\t'temperature_2m': [24.3],\n",
    "\t'stop_distance': [400],\n",
    "\t'cloud_cover': [0],\n",
    "\t'trip_phase_start': [0]\n",
    "}\n",
    "\n",
    "x_test = pd.DataFrame(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb883fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict delay\n",
    "prediction = final_model.predict(x_test)\n",
    "print(f'Predicted delay: {prediction[0]:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8a024",
   "metadata": {},
   "source": [
    "### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, hyperparameters and predictors\n",
    "joblib.dump(final_model, '../models/regression_model.pkl')\n",
    "joblib.dump(cat_pruned_best_params, '../models/best_hyperparams.pkl')\n",
    "joblib.dump(best_features, '../models/best_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e39b53",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
